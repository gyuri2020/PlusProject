{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doctor.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPOGHTHlsfw3U5YXu1pRmee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyuri2020/PlusProject/blob/master/doctor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brj1R0vpm4dx"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aULwA_t9ZRcW"
      },
      "source": [
        "pip install pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUUSZFR-nEms"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUwD0cWO_wbs"
      },
      "source": [
        "**논문 추천**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThDF8nS1mr_S"
      },
      "source": [
        "import math\n",
        "import operator\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import pymysql\n",
        "\n",
        "host = 'database-skku.c6dzc5dnqf69.ap-northeast-2.rds.amazonaws.com'\n",
        "iid ='admin'\n",
        "pw = 'tjdrbsrhkseo123'\n",
        "db_name = 'dongwan'\n",
        "conn = pymysql.connect(host=host, user= iid, password=pw, db=db_name, charset='utf8')\n",
        "\n",
        "curs = conn.cursor(pymysql.cursors.DictCursor)\n",
        "\n",
        "# 데이터 사용 version 1\n",
        "sql = \"\"\"SELECT * FROM public.scholar where meshterm != 'null'\"\"\"\n",
        "sql1 = \"\"\"SELECT * FROM public.all_nor\"\"\"\n",
        "sql2 = \"\"\"SELECT * FROM medii.doctor_data\"\"\"\n",
        "\n",
        "curs.execute(sql)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data = pd.DataFrame(rows)\n",
        "\n",
        "curs.execute(sql1)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data1 = pd.DataFrame(rows)\n",
        "\n",
        "curs.execute(sql2)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data2 = pd.DataFrame(rows)\n",
        "\n",
        "search_paper_list = list()\n",
        "\n",
        "data = data.fillna(' ')\n",
        "print(data.shape)\n",
        "data['text'] = data['meshterm']\n",
        "doctor =pd.read_csv('doctordata.csv', low_memory=False,  encoding=\"UTF-8\")\n",
        "\n",
        "'''\n",
        "doctor = dict()\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    text.lower()\n",
        "\n",
        "    text = text.replace('-', \"HYPHEN\")\n",
        "    text = text.replace('*', \"\")\n",
        "    text = text.replace(' / ', \",\")\n",
        "    text = text.replace(' /', \",\")\n",
        "    text = text.replace('/ ', \",\")\n",
        "    text = text.replace(', ', \",\")\n",
        "    text = text.replace(' ,', \",\")\n",
        "    text = text.replace(' ', \"JUMP\")\n",
        "    text = text.replace('.', \"DOT\")\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words and w != ',' and w != '&':\n",
        "            filtered_sentence.append(w)\n",
        "\n",
        "    filtered = ', '.join(filtered_sentence)\n",
        "\n",
        "    return filtered.lower().strip()\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: preprocess(x))\n",
        "\n",
        "def doctorframe(name, text):\n",
        "\n",
        "    if name in doctor:\n",
        "        doctor[name] = doctor[name] + ', ' + text\n",
        "\n",
        "    else:\n",
        "        doctor[name] = text\n",
        "\n",
        "\n",
        "data.apply(lambda x: doctorframe(x['name_kor'],x['text']),axis =1)\n",
        "\n",
        "tempdoctorframe = pd.DataFrame({'name': doctor.keys(),\n",
        "                             'text': doctor.values()})\n",
        "\n",
        "tempdoctorframe.to_csv('doctordata.csv',index=False, mode='w')\n",
        "'''\n",
        "\n",
        "def get_recommendation(input, option):\n",
        "\n",
        "    input = preprocess(input)\n",
        "    name = list(doctor['name'])\n",
        "    target_index = len(name)\n",
        "    name.append('target')\n",
        "    #print(len(doctor.keys()))\n",
        "\n",
        "    text = list(doctor['text'])\n",
        "    text.append(input)\n",
        "    \n",
        "    #print(input, option)\n",
        "\n",
        "    doctors = pd.DataFrame({'name': name,\n",
        "                            'text': text})\n",
        "    #print(len(doctors['name']))\n",
        "\n",
        "    tfidf_vector = TfidfVectorizer(min_df =3, max_features = 6000)\n",
        "    tfidf_matrix = tfidf_vector.fit_transform(doctors['text']).toarray()\n",
        "\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    cosine_sim_df = pd.DataFrame(cosine_sim, index = doctors.name, columns = doctors.name)\n",
        "    cosine_sim_df.head()\n",
        "\n",
        "    for i in range(len(doctors)):\n",
        "        doctors['text'][i] = doctors['text'][i].replace('jump',' ')\n",
        "        doctors['text'][i] = doctors['text'][i].replace('hyphen', '-')\n",
        "        doctors['text'][i] = doctors['text'][i].replace('dot', '.')\n",
        "\n",
        "    print('-------------------------------------')\n",
        "    print('검색 질병명 : ' + doctors['text'][target_index])\n",
        "\n",
        "    paper_similarity = cosine_sim_df['target']\n",
        "    paper_data = pd.DataFrame({'cosine_similarity': paper_similarity, 'index': np.arange(len(doctors))})\n",
        "\n",
        "    if option == '1':\n",
        "\n",
        "        start = 1\n",
        "        ranking = paper_data.sort_values(by='cosine_similarity', ascending=False)\n",
        "        topn = 6\n",
        "\n",
        "        paper_ids = ranking['index'][start:topn]\n",
        "\n",
        "        std = set(doctors['text'][target_index].split(', '))\n",
        "\n",
        "        count = start\n",
        "        recommendation_doctors = list()\n",
        "\n",
        "        for temp in paper_ids:\n",
        "            bump = dict()\n",
        "            bump['name'] = doctors['name'][temp]\n",
        "            word_target = set(doctors['text'][temp].split(', '))\n",
        "            words = doctors['text'][temp].split(', ')\n",
        "            add_keyword = std & word_target\n",
        "            words_count = {}\n",
        "\n",
        "            for word in words:\n",
        "                if word in words_count:\n",
        "                    words_count[word] += 1\n",
        "                else:\n",
        "                    words_count[word] = 1\n",
        "\n",
        "            sorted_words = sorted([(k, v) for k, v in words_count.items()], key=lambda word_count: -word_count[1])\n",
        "            keyword = [w for w in sorted_words if w[0] in add_keyword]\n",
        "            if (len(keyword) >= 5):\n",
        "                keyword = keyword[0:5]\n",
        "\n",
        "            bump['simul'] = round(ranking.iloc[count]['cosine_similarity'], 3)\n",
        "            bump['keyword'] = keyword\n",
        "            recommendation_doctors.append(bump)\n",
        "            count = count + 1\n",
        "\n",
        "        return recommendation_doctors\n",
        "\n",
        "    if option == '2':\n",
        "\n",
        "        def score(i):\n",
        "\n",
        "            target_name = doctors['name'][i]\n",
        "            person_indexs = data1[data1['name_kor'] == target_name].index\n",
        "\n",
        "            if(len(person_indexs) == 0):\n",
        "                return 0\n",
        "\n",
        "            else:\n",
        "                person_index = person_indexs[0]\n",
        "                return data1['scoresum'][person_index] + paper_data['cosine_similarity'][i]*3\n",
        "\n",
        "        paper_data['total'] = paper_data['index'].apply(lambda x: score(x))\n",
        "\n",
        "        start = 0\n",
        "        ranking = paper_data.sort_values(by='total', ascending=False)\n",
        "        topn = 6\n",
        "\n",
        "        paper_ids = ranking['index'][start:topn]\n",
        "        std = set(doctors['text'][target_index].split(', '))\n",
        "\n",
        "        count = start\n",
        "        recommendation_doctors = list()\n",
        "\n",
        "        for temp in paper_ids:\n",
        "\n",
        "            bump = dict()\n",
        "            bump['name'] = doctors['name'][temp]\n",
        "\n",
        "            if bump['name'] == 'target':\n",
        "                count += 1\n",
        "                continue\n",
        "\n",
        "            person_indexs = data2[data2['name_kor'] == bump['name']].index\n",
        "            person_index = person_indexs[0]\n",
        "            bump['major'] = data2['major'][person_index]\n",
        "\n",
        "            person_indexs = data1[data1['name_kor'] == bump['name']].index\n",
        "            person_index = person_indexs[0]\n",
        "            bump['belong'] = data1['belong'][person_index]\n",
        "            bump['paper_count'] = data1['pubnum'][person_index]\n",
        "            \n",
        "\n",
        "\n",
        "            word_target = set(doctors['text'][temp].split(', '))\n",
        "            words = doctors['text'][temp].split(', ')\n",
        "            add_keyword = std & word_target\n",
        "\n",
        "            words_count = {}\n",
        "            for word in words:\n",
        "                if word in words_count:\n",
        "                    words_count[word] += 1\n",
        "                else:\n",
        "                    words_count[word] = 1\n",
        "\n",
        "            sorted_words = sorted([(k, v) for k, v in words_count.items()], key=lambda word_count: -word_count[1])\n",
        "            keyword = [w[0] for w in sorted_words if w[0] in add_keyword]\n",
        "            if(len(keyword) >= 5):\n",
        "                keyword = keyword[0:5]\n",
        "\n",
        "            bump['total'] = round(ranking.iloc[count]['total'], 3)\n",
        "            bump['simul'] = round(ranking.iloc[count]['cosine_similarity'], 3)\n",
        "            bump['paper_impact'] = data1['scoresum'][person_index]\n",
        "            bump['keyword'] = keyword\n",
        "\n",
        "            recommendation_doctors.append(bump)\n",
        "\n",
        "            count = count +1\n",
        "\n",
        "        return recommendation_doctors\n",
        "\n",
        "\n",
        "\n",
        "input_text = ' '\n",
        "\n",
        "\n",
        "while(input_text != 'exit'):\n",
        "\n",
        "    print('\\n\\n질병명들을 입력해주세요')\n",
        "    input_text = input()\n",
        "    print('옵션을 선택하세요')\n",
        "    print('1. 질병 유사도만 반영 2. 다른 가중치까지 다 반영')\n",
        "    option = input()\n",
        "    count = 1\n",
        "\n",
        "    recom_list = get_recommendation(input_text, option)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    for i in recom_list:\n",
        "        print(str(count) + '순위')\n",
        "        for key, value in i.items():\n",
        "          print(key +' : '+ str(value))\n",
        "        print('---------------------------------------')\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    print('---------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmc-94Oc_-Oi"
      },
      "source": [
        "# **임상시험 + 논문**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESd8dJzXAFwW"
      },
      "source": [
        "**논문 함수**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6weLCs_K_zFo"
      },
      "source": [
        "import math\n",
        "import operator\n",
        "import numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import pymysql\n",
        "\n",
        "host = 'database-skku.c6dzc5dnqf69.ap-northeast-2.rds.amazonaws.com'\n",
        "iid ='admin'\n",
        "pw = 'tjdrbsrhkseo123'\n",
        "db_name = 'dongwan'\n",
        "conn = pymysql.connect(host=host, user= iid, password=pw, db=db_name, charset='utf8')\n",
        "\n",
        "curs = conn.cursor(pymysql.cursors.DictCursor)\n",
        "\n",
        "# 데이터 사용 version 1\n",
        "sql = \"\"\"SELECT * FROM public.scholar where meshterm != 'null'\"\"\"\n",
        "sql1 = \"\"\"SELECT * FROM public.all_nor\"\"\"\n",
        "sql2 = \"\"\"SELECT * FROM medii.doctor_data\"\"\"\n",
        "\n",
        "curs.execute(sql)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data = pd.DataFrame(rows)\n",
        "\n",
        "curs.execute(sql1)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data1 = pd.DataFrame(rows)\n",
        "\n",
        "curs.execute(sql2)\n",
        "rows = curs.fetchall()\n",
        "\n",
        "data2 = pd.DataFrame(rows)\n",
        "\n",
        "search_paper_list = list()\n",
        "\n",
        "data = data.fillna(' ')\n",
        "print(data.shape)\n",
        "data['text'] = data['meshterm']\n",
        "doctor =pd.read_csv('doctordata.csv', low_memory=False,  encoding=\"UTF-8\")\n",
        "\n",
        "\n",
        "def get_recommendation(input):\n",
        "\n",
        "    input = preprocess(input)\n",
        "    name = list(doctor['name'])\n",
        "    target_index = len(name)\n",
        "    name.append('target')\n",
        "\n",
        "    text = list(doctor['text'])\n",
        "    text.append(input)\n",
        "\n",
        "    doctors = pd.DataFrame({'name': name,\n",
        "                            'text': text})\n",
        "\n",
        "    tfidf_vector = TfidfVectorizer(min_df =3, max_features = 6000)\n",
        "    tfidf_matrix = tfidf_vector.fit_transform(doctors['text']).toarray()\n",
        "\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    cosine_sim_df = pd.DataFrame(cosine_sim, index = doctors.name, columns = doctors.name)\n",
        "    cosine_sim_df.head()\n",
        "\n",
        "    for i in range(len(doctors)):\n",
        "        doctors['text'][i] = doctors['text'][i].replace('jump',' ')\n",
        "        doctors['text'][i] = doctors['text'][i].replace('hyphen', '-')\n",
        "        doctors['text'][i] = doctors['text'][i].replace('dot', '.')\n",
        "\n",
        "    print('-------------------------------------')\n",
        "    print('검색 질병명 : ' + doctors['text'][target_index])\n",
        "\n",
        "    paper_similarity = cosine_sim_df['target']\n",
        "    paper_data = pd.DataFrame({'cosine_similarity': paper_similarity, 'index': np.arange(len(doctors))})\n",
        "\n",
        "\n",
        "    def score(i):\n",
        "\n",
        "        target_name = doctors['name'][i]\n",
        "        person_indexs = data1[data1['name_kor'] == target_name].index\n",
        "\n",
        "        if(len(person_indexs) == 0):\n",
        "            return 0\n",
        "\n",
        "        else:\n",
        "            person_index = person_indexs[0]\n",
        "            return data1['scoresum'][person_index] + paper_data['cosine_similarity'][i]*3\n",
        "\n",
        "    paper_data['total'] = paper_data['index'].apply(lambda x: score(x))\n",
        "\n",
        "    start = 0\n",
        "    ranking = paper_data.sort_values(by='total', ascending=False)\n",
        "    topn = 6\n",
        "\n",
        "    paper_ids = ranking['index'][start:topn]\n",
        "    std = set(doctors['text'][target_index].split(', '))\n",
        "\n",
        "    count = start\n",
        "    recommendation_doctors = list()\n",
        "\n",
        "    for temp in paper_ids:\n",
        "\n",
        "        bump = dict()\n",
        "        bump['name'] = doctors['name'][temp]\n",
        "\n",
        "        if bump['name'] == 'target':\n",
        "            count += 1\n",
        "            continue\n",
        "\n",
        "        person_indexs = data2[data2['name_kor'] == bump['name']].index\n",
        "        person_index = person_indexs[0]\n",
        "        bump['major'] = data2['major'][person_index]\n",
        "\n",
        "        person_indexs = data1[data1['name_kor'] == bump['name']].index\n",
        "        person_index = person_indexs[0]\n",
        "        bump['belong'] = data1['belong'][person_index]\n",
        "        bump['paper_count'] = data1['pubnum'][person_index]\n",
        "        \n",
        "\n",
        "\n",
        "        word_target = set(doctors['text'][temp].split(', '))\n",
        "        words = doctors['text'][temp].split(', ')\n",
        "        add_keyword = std & word_target\n",
        "\n",
        "        words_count = {}\n",
        "        for word in words:\n",
        "            if word in words_count:\n",
        "                words_count[word] += 1\n",
        "            else:\n",
        "                words_count[word] = 1\n",
        "\n",
        "        sorted_words = sorted([(k, v) for k, v in words_count.items()], key=lambda word_count: -word_count[1])\n",
        "        keyword = [w[0] for w in sorted_words if w[0] in add_keyword]\n",
        "        if(len(keyword) >= 5):\n",
        "            keyword = keyword[0:5]\n",
        "\n",
        "        bump['total'] = round(ranking.iloc[count]['total'], 3)\n",
        "        bump['simul'] = round(ranking.iloc[count]['cosine_similarity'], 3)\n",
        "        bump['paper_impact'] = data1['scoresum'][person_index]\n",
        "        bump['keyword'] = keyword\n",
        "\n",
        "        recommendation_doctors.append(bump)\n",
        "\n",
        "        count = count +1\n",
        "\n",
        "    return recommendation_doctors\n",
        "\n",
        "\n",
        "\n",
        "input_text = ' '\n",
        "\n",
        "\n",
        "while(input_text != 'exit'):\n",
        "\n",
        "    print('\\n\\n질병명들을 입력해주세요')\n",
        "    input_text = input()\n",
        "    #print('옵션을 선택하세요')\n",
        "    #print('1. 질병 유사도만 반영 2. 다른 가중치까지 다 반영')\n",
        "    count = 1\n",
        "\n",
        "    recom_list = get_recommendation(input_text)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    for i in recom_list:\n",
        "        print(str(count) + '순위')\n",
        "        for key, value in i.items():\n",
        "          print(key +' : '+ str(value))\n",
        "        print('---------------------------------------')\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    print('---------------------------------------')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}